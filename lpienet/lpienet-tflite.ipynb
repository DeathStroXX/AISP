{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install keras-flops","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:54:04.173831Z","iopub.execute_input":"2023-04-05T18:54:04.174871Z","iopub.status.idle":"2023-04-05T18:54:15.044401Z","shell.execute_reply.started":"2023-04-05T18:54:04.174761Z","shell.execute_reply":"2023-04-05T18:54:15.043000Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport gc\n\n#import keras\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\nfrom tensorflow.keras.layers import *\nimport tensorflow_addons as tfa\nimport keras_flops\nfrom keras_flops import get_flops\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:54:15.050542Z","iopub.execute_input":"2023-04-05T18:54:15.050859Z","iopub.status.idle":"2023-04-05T18:54:17.159720Z","shell.execute_reply.started":"2023-04-05T18:54:15.050822Z","shell.execute_reply":"2023-04-05T18:54:17.158587Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"def keras2tflite (model, name, fp16=False):\n    print ('converting...')\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    if fp16:\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.target_spec.supported_types = [tf.float16, tf.int8]\n\n    # Be very careful here:\n    # \"experimental_new_converter\" is enabled by default in TensorFlow 2.2+. However, using the new MLIR TFLite\n    # converter might result in corrupted / incorrect TFLite models for some particular architectures. Therefore, the\n    # best option is to perform the conversion using both the new and old converter and check the results in each case:\n    #converter.target_ops= [TFLITE_BUILTINS,SELECT_TF_OPS]\n    #converter.target_spec.supported_ops = [\n    #    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n    #    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n    #]\n    \n    converter.experimental_new_converter = False\n    tflite_model = converter.convert()\n    open(f\"{name}\", \"wb\").write(tflite_model)\n    print ('saved!')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:54:32.924600Z","iopub.execute_input":"2023-04-05T18:54:32.925431Z","iopub.status.idle":"2023-04-05T18:54:32.933173Z","shell.execute_reply.started":"2023-04-05T18:54:32.925386Z","shell.execute_reply":"2023-04-05T18:54:32.932054Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 400, 400, 3\nHR_HEIGHT,  HR_WIDTH,  HR_CHANNELS  = 1080, 1920, 3\nLR_HEIGHT,  LR_WIDTH,  LR_CHANNELS  = 256, 256, 3\n\nconv_activation = tf.keras.layers.LeakyReLU()\nconv_activation = tf.keras.activations.elu\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True, bn=False, dilation=1):\n    x = tf.keras.layers.Conv2D(filters, size, strides=strides, padding=padding, dilation_rate=dilation)(x) #name='{}_conv'.format(name)\n    if bn:\n        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n    if activation:\n        x = conv_activation(x)\n    return x\n\ndef residual_subblock(blockInput,num_filters):\n    x = convolution_block(blockInput, num_filters, (3,3) ,activation=True)\n    x = convolution_block(x, num_filters, (3,3), activation=True)\n    x = tf.keras.layers.Add()([x, blockInput])\n    #x = conv_activation(x)\n    return x\n\ndef inverted_linear_residual_block(x, expand=64, squeeze=16):\n    m = Conv2D(expand, (1,1), activation='elu', strides=(1,1), padding='same')(x)\n    m = DepthwiseConv2D((3,3), activation='elu', strides=(1,1), padding='same')(m)\n    m = Conv2D(squeeze, (1,1))(m)\n    return Add()([m, x])\n\ndef inverted_proj_block(x, proj=16):\n    m = DepthwiseConv2D((3,3),  activation='elu', strides=(1,1), padding='same')(x)\n    m = Conv2D(proj, (1,1))(m)\n    return m\n\ndef CALayer(blockInput,num_filters):\n    '''\n    Dilated Attention Block (DAB)\n    '''\n    y = blockInput\n    filtersCount = blockInput.shape[-1]\n    x0 = convolution_block(y,num_filters,(3,3),activation=True,dilation=1)\n    x1 = convolution_block(y,num_filters,(3,3),activation=True,dilation=2)\n    x2 = convolution_block(y,num_filters,(3,3),activation=True,dilation=4)\n    out = Concatenate(axis=3)([x0,x1,x2])\n\n    sg = tf.keras.layers.Conv2D(filtersCount, (3,3), strides=1, padding=\"same\")(out)\n    sg = Activation(\"sigmoid\")(sg)\n    return  tf.keras.layers.Multiply()([blockInput,sg])\n\n\ndef residual_dense_attention(blockInput, num_filters=16):\n    '''\n    RDB block with attention DAB\n    '''\n    count = 3\n    li = [blockInput]\n    pas= convolution_block(blockInput, num_filters,size=(3,3),strides=(1,1))\n    for i in range(2 , count+1):\n        li.append(pas)\n        out = tf.keras.layers.Concatenate(axis = 3)(li) # conctenated out put\n        pas = convolution_block(out,num_filters,size=(3,3),strides=(1,1))\n        pas = residual_subblock(pas,num_filters)\n        #pas = inverted_linear_residual_block(pas, expand=num_filters*2, squeeze=num_filters)\n    \n    li.append(pas)\n    out = Concatenate(axis=3)(li)\n    out = tf.keras.layers.Conv2D(num_filters, (3,3), strides=(1,1), padding=\"same\", activation='relu')(out)\n    return out\n\ndef DAB (x, dim=64):\n    \n    inputs = x\n    \n    for i in range(2):\n        x = tf.keras.layers.Conv2D(dim, (3,3), strides=(1,1), padding=\"same\", activation='relu')(x)\n    \n    shortcut = x\n    \n    gap = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(x)\n    gmp = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(x)\n    \n    ## spatial attention\n    gap_gmp = Concatenate(axis=3)([gap, gmp])\n    gap_gmp = tf.keras.layers.Conv2D(dim, (3,3), strides=(1,1), \n                               padding=\"same\", \n                               activation='sigmoid')(gap_gmp)\n    \n    spatial_attention = multiply([shortcut, gap_gmp])\n    \n    ## channel attention\n    x1 = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n                               padding=\"same\", \n                               activation='relu')(gap)\n    x1 = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n                               padding=\"same\", \n                               activation='sigmoid')(x1)\n    \n    channel_attention = multiply([shortcut, x1])\n    \n    \n    attention = Concatenate(axis=3)([spatial_attention, channel_attention])\n    x2 = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n                               padding=\"same\", \n                               activation='relu')(attention)\n    \n    #input_project = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n    #                           padding=\"same\", \n    #                           activation='relu')(inputs)\n    \n    out = Add()([inputs, x2])\n    return out\n    \n\ndef RRG(x, kernel_size, reduction, n_feats=64, num_dab=8):\n    '''Recursive Residual Group\n    source: https://github.com/swz30/CycleISP'''\n    shortcut = x\n    for _ in range(num_dab):\n        x = DAB (x,dim=n_feats)\n        \n    x = tf.keras.layers.Conv2D(n_feats, (3,3), strides=(1,1), padding=\"same\", activation='relu')(x)\n    out = out = Add()([shortcut, x])\n    return out\n\ndef basic_encoder(blockInput,num_filters,activation=True):\n    x = convolution_block(blockInput, num_filters, (3,3) ,activation=True)\n    x = convolution_block(x, num_filters, (3,3), activation=True)\n    x = tf.keras.layers.Add()([x, convolution_block(blockInput, num_filters, (3,3), activation=True)])\n    if activation:\n        x = tf.keras.layers.LeakyReLU()(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:54:40.922938Z","iopub.execute_input":"2023-04-05T18:54:40.923395Z","iopub.status.idle":"2023-04-05T18:54:41.132283Z","shell.execute_reply.started":"2023-04-05T18:54:40.923355Z","shell.execute_reply":"2023-04-05T18:54:41.131127Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Main Blocks","metadata":{}},{"cell_type":"code","source":"gelu = tf.keras.activations.gelu\nselu = tf.keras.activations.selu\nelu = tf.keras.activations.elu\n\ndef attention_block (x, dim=16):\n    \n    inputs = x\n    \n    #x = tf.keras.layers.Conv2D(dim, (3,3), strides=(1,1), \n    #                           padding=\"same\", \n    #                           activation='relu')(x)\n    \n    shortcut = x\n    \n    gap = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(x)\n    gmp = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(x)\n    \n    ## spatial attention\n    gap_gmp = Concatenate(axis=3)([gap, gmp])\n    gap_gmp = tf.keras.layers.Conv2D(dim, (3,3), strides=(1,1), \n                               padding=\"same\", \n                               activation='sigmoid')(gap_gmp)\n    \n    spatial_attention = multiply([shortcut, gap_gmp])\n    \n    ## channel attention\n    x1 = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n                               padding=\"same\", \n                               activation='relu')(gap)\n    x1 = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n                               padding=\"same\", \n                               activation='sigmoid')(x1)\n    \n    channel_attention = multiply([shortcut, x1])\n    \n    \n    attention = Concatenate(axis=3)([spatial_attention, channel_attention])\n    x2 = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n                               padding=\"same\", \n                               activation=None)(attention)\n    \n    #input_project = tf.keras.layers.Conv2D(dim, (1,1), strides=(1,1), \n    #                           padding=\"same\", \n    #                           activation=None)(inputs)\n    \n    out = Add()([inputs, x2])\n    return out\n    \n\ndef RRG(x,dim=16):\n    '''Recursive Residual Group\n    source: https://github.com/swz30/CycleISP'''\n    x = attention_block(x,dim)\n    return out\n\ndef flatten(x) :\n    return tf.layers.flatten(x)\n\ndef hw_flatten(x) :\n    return tf.reshape(x, shape=[x.shape[0], -1, x.shape[-1]])\n\ndef sagan_block(x, channels):\n    f = Conv2D(channels, (1,1), activation=None, strides=(1,1), padding='same')(x)\n    g = Conv2D(channels, (1,1), activation=None, strides=(1,1), padding='same')(x)\n    h = Conv2D(channels, (1,1), activation=None, strides=(1,1), padding='same')(x)\n\n    f = tf.transpose(f)\n    att_map = f*g\n    att_map = tf.keras.activations.softmax (att_map)\n    fe = att_map * h\n    fe = Conv2D(channels, (1,1), activation='sigmoid', strides=(1,1), padding='same')(fe)\n    return fe\n\ndef sat(x, channels=3):\n    f = Conv2D(channels, (7,7), activation='relu', strides=(1,1), padding='same')(x)\n    f = Conv2D(channels, (5,5), activation='relu', strides=(1,1), padding='same')(f)\n    f = Conv2D(channels, (3,3), activation='sigmoid', strides=(1,1), padding='same')(f)\n    return x * f\n\ndef inv_block(x, channels=3):\n    m = x\n    m = Conv2D(channels, (1,1), activation =None, strides=(1,1), padding='same')(m)\n    m = DepthwiseConv2D((3,3), activation=None, strides=(1,1), padding='same')(m)\n    m = elu(m)\n    m = Conv2D(channels, (1,1))(m)\n    \n    x = Conv2D(channels, (1,1), activation ='relu', strides=(1,1), padding='same')(x)\n    y = Add()([m, x])\n    return y\n    \ndef baseblock(x, channels=32):\n    #m = LayerNormalization()(x)\n    m = x\n    m = Conv2D(channels, (1,1), activation =None, strides=(1,1), padding='same')(m)\n    m = DepthwiseConv2D((3,3), activation=None, strides=(1,1), padding='same')(m)\n    m = elu(m)\n    m = Conv2D(channels, (1,1))(m)\n    y = Add()([m, x])\n    #m = LayerNormalization()(m)\n    m = Conv2D(channels, (1,1), activation= None, strides=(1,1), padding='same')(m)\n    m = elu(m)\n    m = Conv2D(channels, (1,1), activation= None, strides=(1,1), padding='same')(m)\n    m = Add()([m, y])\n    return m","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:54:41.982449Z","iopub.execute_input":"2023-04-05T18:54:41.982863Z","iopub.status.idle":"2023-04-05T18:54:42.006944Z","shell.execute_reply.started":"2023-04-05T18:54:41.982827Z","shell.execute_reply":"2023-04-05T18:54:42.005898Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def build_ours(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),learning_rate=0.001):\n\n    encoder_dim = [16, 32, 64]\n    encoder_fes = []\n    decoder_dim = [32, 16]\n    enc_dec_cat = [1, 0]\n    \n    inputs = tf.keras.layers.Input(input_shape)\n    x = inputs\n    \n    for e in range(len(encoder_dim)):\n        x = tf.keras.layers.Conv2D(encoder_dim[e], (3, 3), activation=\"relu\", padding=\"same\")(x)\n        x = baseblock(x,encoder_dim[e])\n        x = baseblock(x,encoder_dim[e])\n        x = attention_block(x,encoder_dim[e])\n        encoder_fes.append(x)\n        print ('e', e, x.shape)\n        if e != (len(encoder_dim)-1):\n            x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    for d in range(len(decoder_dim)):\n        x = tf.keras.layers.Conv2D(decoder_dim[d], (3, 3), activation=\"relu\", padding=\"same\")(x)\n        x = baseblock(x,decoder_dim[d])\n        x = baseblock(x,decoder_dim[d])\n        x = attention_block(x,decoder_dim[d])\n        print ('d', d, x.shape)\n        x = tf.keras.layers.UpSampling2D(size=(2,2),interpolation='bilinear')(x)\n        #x = Conv2D(x.shape[-1], (3,3), activation =None, strides=(1,1), padding='same')(x)\n        x = tf.keras.layers.Concatenate()([x, encoder_fes[enc_dec_cat[d]]])\n    \n    x = inv_block(x,3)\n    x = sat(x)\n    x = x + inputs\n    \n    model = tf.keras.models.Model(inputs=[inputs], outputs=[x])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mse')\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:54:43.025473Z","iopub.execute_input":"2023-04-05T18:54:43.026418Z","iopub.status.idle":"2023-04-05T18:54:43.039313Z","shell.execute_reply.started":"2023-04-05T18:54:43.026374Z","shell.execute_reply":"2023-04-05T18:54:43.038006Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\nmodel = build_ours(input_shape=(3840 , 2160, 3 ))\nprint (model.count_params() / 1_000_000, 'M params.')\n#model.summary()\n\nflops = get_flops(model, batch_size=1)\nprint(f\"FLOPS: {flops / 1e9} G\")\n\nMODEL_NAME = 'wours_4k.tflite'\nkeras2tflite(model, name=MODEL_NAME, fp16=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:55:02.787052Z","iopub.execute_input":"2023-04-05T18:55:02.787507Z","iopub.status.idle":"2023-04-05T18:55:25.559245Z","shell.execute_reply.started":"2023-04-05T18:55:02.787467Z","shell.execute_reply":"2023-04-05T18:55:25.557842Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"e 0 (None, 3840, 2160, 16)\ne 1 (None, 1920, 1080, 32)\ne 2 (None, 960, 540, 64)\nd 0 (None, 960, 540, 32)\nd 1 (None, 1920, 1080, 16)\n0.133652 M params.\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/310.46b flops)\n  model/conv2d_52/Conv2D (38.22b/38.22b flops)\n  model/conv2d_39/Conv2D (19.11b/19.11b flops)\n  model/conv2d_13/Conv2D (19.11b/19.11b flops)\n  model/conv2d_26/Conv2D (19.11b/19.11b flops)\n  model/conv2d_38/Conv2D (8.49b/8.49b flops)\n  model/conv2d_25/Conv2D (8.49b/8.49b flops)\n  model/conv2d_12/Conv2D (8.49b/8.49b flops)\n  model/conv2d_68/Conv2D (7.32b/7.32b flops)\n  model/conv2d/Conv2D (7.17b/7.17b flops)\n  model/conv2d_9/Conv2D (4.78b/4.78b flops)\n  model/conv2d_14/Conv2D (4.25b/4.25b flops)\n  model/conv2d_1/Conv2D (4.25b/4.25b flops)\n  model/conv2d_29/Conv2D (4.25b/4.25b flops)\n  model/conv2d_6/Conv2D (4.25b/4.25b flops)\n  model/conv2d_11/Conv2D (4.25b/4.25b flops)\n  model/conv2d_3/Conv2D (4.25b/4.25b flops)\n  model/conv2d_30/Conv2D (4.25b/4.25b flops)\n  model/conv2d_5/Conv2D (4.25b/4.25b flops)\n  model/conv2d_28/Conv2D (4.25b/4.25b flops)\n  model/conv2d_15/Conv2D (4.25b/4.25b flops)\n  model/conv2d_16/Conv2D (4.25b/4.25b flops)\n  model/conv2d_27/Conv2D (4.25b/4.25b flops)\n  model/conv2d_17/Conv2D (4.25b/4.25b flops)\n  model/conv2d_18/Conv2D (4.25b/4.25b flops)\n  model/conv2d_19/Conv2D (4.25b/4.25b flops)\n  model/conv2d_2/Conv2D (4.25b/4.25b flops)\n  model/conv2d_24/Conv2D (4.25b/4.25b flops)\n  model/conv2d_20/Conv2D (4.25b/4.25b flops)\n  model/conv2d_21/Conv2D (4.25b/4.25b flops)\n  model/conv2d_37/Conv2D (4.25b/4.25b flops)\n  model/conv2d_4/Conv2D (4.25b/4.25b flops)\n  model/conv2d_31/Conv2D (4.25b/4.25b flops)\n  model/conv2d_7/Conv2D (4.25b/4.25b flops)\n  model/conv2d_34/Conv2D (4.25b/4.25b flops)\n  model/conv2d_32/Conv2D (4.25b/4.25b flops)\n  model/conv2d_8/Conv2D (4.25b/4.25b flops)\n  model/conv2d_33/Conv2D (4.25b/4.25b flops)\n  model/conv2d_69/Conv2D (3.73b/3.73b flops)\n  model/conv2d_22/Conv2D (2.39b/2.39b flops)\n  model/depthwise_conv2d_1/depthwise (2.39b/2.39b flops)\n  model/depthwise_conv2d/depthwise (2.39b/2.39b flops)\n  model/conv2d_64/Conv2D (2.12b/2.12b flops)\n  model/conv2d_51/Conv2D (2.12b/2.12b flops)\n  model/conv2d_65/Conv2D (1.59b/1.59b flops)\n  model/conv2d_67/Conv2D (1.59b/1.59b flops)\n  model/conv2d_70/Conv2D (1.34b/1.34b flops)\n  model/depthwise_conv2d_2/depthwise (1.19b/1.19b flops)\n  model/depthwise_conv2d_3/depthwise (1.19b/1.19b flops)\n  model/conv2d_35/Conv2D (1.19b/1.19b flops)\n  model/conv2d_61/Conv2D (1.19b/1.19b flops)\n  model/conv2d_46/Conv2D (1.06b/1.06b flops)\n  model/conv2d_42/Conv2D (1.06b/1.06b flops)\n  model/conv2d_47/Conv2D (1.06b/1.06b flops)\n  model/conv2d_43/Conv2D (1.06b/1.06b flops)\n  model/conv2d_41/Conv2D (1.06b/1.06b flops)\n  model/conv2d_40/Conv2D (1.06b/1.06b flops)\n  model/conv2d_45/Conv2D (1.06b/1.06b flops)\n  model/conv2d_44/Conv2D (1.06b/1.06b flops)\n  model/conv2d_56/Conv2D (1.06b/1.06b flops)\n  model/conv2d_50/Conv2D (1.06b/1.06b flops)\n  model/conv2d_63/Conv2D (1.06b/1.06b flops)\n  model/conv2d_60/Conv2D (1.06b/1.06b flops)\n  model/conv2d_59/Conv2D (1.06b/1.06b flops)\n  model/conv2d_58/Conv2D (1.06b/1.06b flops)\n  model/conv2d_57/Conv2D (1.06b/1.06b flops)\n  model/conv2d_55/Conv2D (1.06b/1.06b flops)\n  model/conv2d_54/Conv2D (1.06b/1.06b flops)\n  model/conv2d_53/Conv2D (1.06b/1.06b flops)\n  model/depthwise_conv2d_8/depthwise (597.20m/597.20m flops)\n  model/depthwise_conv2d_9/depthwise (597.20m/597.20m flops)\n  model/conv2d_48/Conv2D (597.20m/597.20m flops)\n  model/depthwise_conv2d_5/depthwise (597.20m/597.20m flops)\n  model/depthwise_conv2d_4/depthwise (597.20m/597.20m flops)\n  model/depthwise_conv2d_10/depthwise (447.90m/447.90m flops)\n  model/depthwise_conv2d_7/depthwise (298.60m/298.60m flops)\n  model/depthwise_conv2d_6/depthwise (298.60m/298.60m flops)\n  model/conv2d_10/Conv2D (265.42m/265.42m flops)\n  model/conv2d_66/Conv2D (149.30m/149.30m flops)\n  model/lambda/Mean (132.71m/132.71m flops)\n  model/depthwise_conv2d_1/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_4/BiasAdd (132.71m/132.71m flops)\n  model/depthwise_conv2d/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_9/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_8/BiasAdd (132.71m/132.71m flops)\n  model/multiply_1/mul (132.71m/132.71m flops)\n  model/conv2d_7/BiasAdd (132.71m/132.71m flops)\n  model/multiply/mul (132.71m/132.71m flops)\n  model/conv2d_6/BiasAdd (132.71m/132.71m flops)\n  model/add/add (132.71m/132.71m flops)\n  model/add_1/add (132.71m/132.71m flops)\n  model/max_pooling2d/MaxPool (132.71m/132.71m flops)\n  model/conv2d_5/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_11/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_2/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_23/Conv2D (132.71m/132.71m flops)\n  model/add_2/add (132.71m/132.71m flops)\n  model/add_3/add (132.71m/132.71m flops)\n  model/conv2d_10/BiasAdd (132.71m/132.71m flops)\n  model/add_4/add (132.71m/132.71m flops)\n  model/conv2d_1/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_12/BiasAdd (132.71m/132.71m flops)\n  model/conv2d/BiasAdd (132.71m/132.71m flops)\n  model/conv2d_3/BiasAdd (132.71m/132.71m flops)\n  model/lambda_1/Max (124.42m/124.42m flops)\n  model/conv2d_14/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_24/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_23/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_13/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_22/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_18/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_21/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_15/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_20/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_16/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_17/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_19/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_62/Conv2D (66.36m/66.36m flops)\n  model/conv2d_25/BiasAdd (66.36m/66.36m flops)\n  model/max_pooling2d_1/MaxPool (66.36m/66.36m flops)\n  model/add_9/add (66.36m/66.36m flops)\n  model/add_8/add (66.36m/66.36m flops)\n  model/add_7/add (66.36m/66.36m flops)\n  model/add_6/add (66.36m/66.36m flops)\n  model/add_5/add (66.36m/66.36m flops)\n  model/multiply_2/mul (66.36m/66.36m flops)\n  model/multiply_3/mul (66.36m/66.36m flops)\n  model/depthwise_conv2d_2/BiasAdd (66.36m/66.36m flops)\n  model/depthwise_conv2d_3/BiasAdd (66.36m/66.36m flops)\n  model/conv2d_36/Conv2D (66.36m/66.36m flops)\n  model/lambda_2/Mean (66.36m/66.36m flops)\n  model/lambda_3/Max (64.28m/64.28m flops)\n  model/add_21/add (33.18m/33.18m flops)\n  model/conv2d_31/BiasAdd (33.18m/33.18m flops)\n  model/depthwise_conv2d_8/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_32/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_33/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_34/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_56/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_35/BiasAdd (33.18m/33.18m flops)\n  model/add_12/add (33.18m/33.18m flops)\n  model/add_24/add (33.18m/33.18m flops)\n  model/add_23/add (33.18m/33.18m flops)\n  model/add_22/add (33.18m/33.18m flops)\n  model/add_11/add (33.18m/33.18m flops)\n  model/add_20/add (33.18m/33.18m flops)\n  model/multiply_4/mul (33.18m/33.18m flops)\n  model/lambda_4/Mean (33.18m/33.18m flops)\n  model/add_13/add (33.18m/33.18m flops)\n  model/multiply_8/mul (33.18m/33.18m flops)\n  model/conv2d_36/BiasAdd (33.18m/33.18m flops)\n  model/multiply_9/mul (33.18m/33.18m flops)\n  model/multiply_5/mul (33.18m/33.18m flops)\n  model/depthwise_conv2d_4/BiasAdd (33.18m/33.18m flops)\n  model/add_14/add (33.18m/33.18m flops)\n  model/depthwise_conv2d_5/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_49/Conv2D (33.18m/33.18m flops)\n  model/conv2d_57/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_55/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_58/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_54/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_59/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_53/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_52/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_38/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_60/BiasAdd (33.18m/33.18m flops)\n  model/lambda_8/Mean (33.18m/33.18m flops)\n  model/conv2d_61/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_30/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_62/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_63/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_26/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_64/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_27/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_37/BiasAdd (33.18m/33.18m flops)\n  model/conv2d_28/BiasAdd (33.18m/33.18m flops)\n  model/add_10/add (33.18m/33.18m flops)\n  model/conv2d_29/BiasAdd (33.18m/33.18m flops)\n  model/depthwise_conv2d_9/BiasAdd (33.18m/33.18m flops)\n  model/lambda_5/Max (32.66m/32.66m flops)\n  model/lambda_9/Max (31.10m/31.10m flops)\n  model/conv2d_70/BiasAdd (24.88m/24.88m flops)\n  model/tf.math.multiply/Mul (24.88m/24.88m flops)\n  model/tf.__operators__.add/AddV2 (24.88m/24.88m flops)\n  model/depthwise_conv2d_10/BiasAdd (24.88m/24.88m flops)\n  model/add_25/add (24.88m/24.88m flops)\n  model/conv2d_69/BiasAdd (24.88m/24.88m flops)\n  model/conv2d_68/BiasAdd (24.88m/24.88m flops)\n  model/conv2d_67/BiasAdd (24.88m/24.88m flops)\n  model/conv2d_66/BiasAdd (24.88m/24.88m flops)\n  model/conv2d_65/BiasAdd (24.88m/24.88m flops)\n  model/multiply_7/mul (16.59m/16.59m flops)\n  model/conv2d_46/BiasAdd (16.59m/16.59m flops)\n  model/lambda_6/Mean (16.59m/16.59m flops)\n  model/multiply_6/mul (16.59m/16.59m flops)\n  model/depthwise_conv2d_6/BiasAdd (16.59m/16.59m flops)\n  model/add_15/add (16.59m/16.59m flops)\n  model/add_16/add (16.59m/16.59m flops)\n  model/add_17/add (16.59m/16.59m flops)\n  model/add_18/add (16.59m/16.59m flops)\n  model/add_19/add (16.59m/16.59m flops)\n  model/conv2d_39/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_40/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_41/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_42/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_43/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_44/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_45/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_47/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_48/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_49/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_50/BiasAdd (16.59m/16.59m flops)\n  model/conv2d_51/BiasAdd (16.59m/16.59m flops)\n  model/depthwise_conv2d_7/BiasAdd (16.59m/16.59m flops)\n  model/lambda_7/Max (16.07m/16.07m flops)\n  model/up_sampling2d/mul (2/2 flops)\n  model/up_sampling2d_1/mul (2/2 flops)\n\n======================End of Report==========================\nFLOPS: 310.462502404 G\nconverting...\nsaved!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}